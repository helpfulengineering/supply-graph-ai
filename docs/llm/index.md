# LLM Integration

The Open Hardware Manager (OHM) includes Large Language Model (LLM) integration for OKH manifest generation and facility matching. This section covers configuration, API endpoints, CLI commands, and usage examples.

## Overview

The LLM integration provides:

- **Multi-Provider Support**: Anthropic Claude, OpenAI GPT, Azure OpenAI, AWS Bedrock, Google Vertex AI (Gemini), and local models
- **Intelligent Provider Selection**: Automatic provider selection with environment variables and CLI overrides
- **Intelligent Generation**: Enhanced OKH manifest generation with context-aware analysis
- **Smart Matching**: Improved facility matching using natural language understanding
- **Cost Management**: Built-in cost tracking and optimization
- **Fallback Mechanisms**: Automatic provider switching for reliability

## Quick Start

### 1. Configuration

Set your API keys in environment variables:

```bash
# Anthropic Claude (recommended)
export ANTHROPIC_API_KEY="your_anthropic_key"

# OpenAI GPT
export OPENAI_API_KEY="your_openai_key"

# Azure OpenAI Service
export AZURE_OPENAI_API_KEY="your_azure_key"
export AZURE_OPENAI_ENDPOINT="https://your-resource.openai.azure.com"
export AZURE_OPENAI_DEPLOYMENT_ID="your-deployment-name"
export AZURE_OPENAI_API_VERSION="2024-05-01-preview"  # Optional

# AWS Bedrock
export AWS_BEDROCK_API_KEY="your_bedrock_key"  # Optional if using AWS credentials
export AWS_BEDROCK_REGION="us-east-1"
# OR use AWS credentials (access keys or IAM role)
export AWS_ACCESS_KEY_ID="your_access_key"
export AWS_SECRET_ACCESS_KEY="your_secret_key"

# Google Cloud Vertex AI
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/service-account.json"
export GOOGLE_CLOUD_PROJECT="your-project-id"
export GOOGLE_CLOUD_LOCATION="us-central1"  # Optional
export VERTEX_AI_MODEL_ID="gemini-1.5-pro"  # Optional

# Local Ollama (optional)
export OLLAMA_BASE_URL="http://localhost:11434"

# Optional: Set default provider
export LLM_PROVIDER="anthropic"
export LLM_MODEL="claude-sonnet-4-5-20250929"
```

### 2. Basic Usage

```bash
# Generate OKH manifest with automatic provider selection
ohm llm generate-okh https://github.com/example/project

# Override provider via CLI flag
ohm llm generate-okh https://github.com/example/project --provider openai
ohm llm generate-okh https://github.com/example/project --provider azure_openai
ohm llm generate-okh https://github.com/example/project --provider aws_bedrock
ohm llm generate-okh https://github.com/example/project --provider google

# Match facilities with LLM enhancement
ohm llm match requirements.okh.json facilities.okw.json

# Check available providers
ohm llm providers info
```

### 3. API Usage

```python
from src.core.llm.provider_selection import create_llm_service_with_selection

# Create LLM service with automatic provider selection
llm_service = await create_llm_service_with_selection()

# Generate content
response = await llm_service.generate(
    prompt="Analyze this hardware project...",
    config=LLMRequestConfig(max_tokens=4000)
)

print(f"Generated by: {response.metadata.provider}")
print(f"Cost: ${response.metadata.cost:.6f}")
```

## Provider Selection System

The LLM integration includes an intelligent provider selection system that automatically chooses the best available provider based on:

### Selection Priority

1. **Command Line Flags** (highest priority)
   ```bash
   ohm llm generate "Hello" --provider anthropic --model claude-sonnet-4-5-20250929
   ohm llm generate "Hello" --provider azure_openai --model gpt-35-turbo
   ohm llm generate "Hello" --provider aws_bedrock --model anthropic.claude-3-5-sonnet-20241022-v2:0
   ohm llm generate "Hello" --provider google --model gemini-1.5-pro
   ```

2. **Environment Variables**
   ```bash
   # Available provider types: anthropic, openai, azure_openai, aws_bedrock, google, local
   export LLM_PROVIDER=anthropic
   export LLM_MODEL=claude-sonnet-4-5-20250929
   ohm llm generate "Hello"
   ```

3. **Auto-Detection** (based on available API keys)
   ```bash
   ohm llm generate "Hello"  # Automatically selects best available provider
   ```

4. **Default Fallback** (lowest priority)
   - Anthropic Claude (if API key available)
   - OpenAI GPT (if API key available)
   - Google Vertex AI (if credentials available)
   - Azure OpenAI (if configured)
   - AWS Bedrock (if credentials available)
   - Local Ollama (if service running)

### Provider Information

Check available providers and their status:

```bash
ohm llm providers info
```

This shows:
- ✅ Available providers with API keys
- ❌ Unavailable providers (missing API keys)
- Default models for each provider
- Environment variable names

## Documentation Sections

- [Configuration](configuration.md) - LLM provider setup and configuration
- [API Reference](api.md) - REST API endpoints for LLM operations
- [CLI Commands](cli.md) - Command-line interface for LLM features
- [Quick Start](llm-quick-start.md) - Getting started with using the LLM Service
- [LLM Service](llm-service.md) - Complete docs on LLM Service
- [Generation Layer](generation.md) - OKH manifest generation with LLM
- [Matching Layer](../matching/index.md) - Facility matching with LLM enhancement
- [Examples](examples.md) - Usage examples and best practices

## Architecture

The LLM integration follows a modular architecture:

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Generation    │    │   Matching      │    │   API/CLI       │
│   Layer         │    │   Layer         │    │   Interface     │
└─────────┬───────┘    └─────────┬───────┘    └─────────┬───────┘
          │                      │                      │
          └──────────────────────┼──────────────────────┘
                                 │
                    ┌─────────────┴─────────────┐
                    │      LLM Service          │
                    │   (Provider Management)   │
                    └─────────────┬─────────────┘
                                  │
                    ┌─────────────┴─────────────┐
                    │    LLM Providers          │
                    │  Anthropic | OpenAI |     │
                    │  Azure OpenAI | AWS       │
                    │  Bedrock | Google | Local │
                    └───────────────────────────┘
```

## Features

### Multi-Provider Support
- **Anthropic Claude**: High-quality analysis and generation (recommended)
- **OpenAI GPT**: Fast and cost-effective processing
- **Azure OpenAI**: Enterprise-grade OpenAI models on Azure infrastructure
- **AWS Bedrock**: Unified API for multiple foundation models (Claude, Llama, Titan, etc.)
- **Google Vertex AI**: Google's Gemini models with enterprise features
- **Local Models**: Ollama and other local LLM support for offline use

### Intelligent Processing
- **Context-Aware Analysis**: Understands project structure and content
- **Schema Compliance**: Generates OKH manifests that follow standards
- **Quality Scoring**: Provides confidence scores for generated content
- **Error Recovery**: Graceful handling of API failures

### Cost Management
- **Usage Tracking**: Monitor token usage and costs
- **Budget Controls**: Set limits on per-request costs
- **Provider Optimization**: Choose most cost-effective providers
- **Analytics**: Detailed usage reports and insights

## Getting Help

- **Documentation**: Browse the sections below for detailed information
- **Examples**: See [Examples](examples.md) for common use cases
- **API Reference**: Check [API Reference](api.md) for endpoint details
- **CLI Help**: Use `ohm --help` for command-line assistance

## Next Steps

1. [Configure LLM providers](configuration.md)
2. [Try the CLI commands](cli.md)
3. [Explore the API](api.md)
4. [See examples](examples.md)
